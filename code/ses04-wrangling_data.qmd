---
title: "Into the tidyverse, Session 4"
subtitle: "Wrangling data"
author: "Jae-Young Son"
date: "2020-11-11"
date-modified: "2022-12-01"
theme:
  light: flatly
  dark: darkly
format:
  html:
    toc: true
    toc-location: left
    code-fold: show
    code-line-numbers: true
    embed-resources: true
---

# Introduction

Let me clue you into one of the dirty secrets of analyzing data: performing statistical tests only constitutes a tiny fraction of the total time you'll spend working with data. Most of the time, you'll instead be trying to **wrangle** your data into the right format so that you can do whatever statistical tests you want.

Just like last time, we'll start by learning some more fundamental prerequisites about programming. Then, you'll be required to review some concepts from past tutorials (and if you find the review challenging, you may need to revisit past tutorials to refresh your memory).

::: callout-note
Before we start, you should create a new R script inside the `sandbox` folder, where you'll write code corresponding to this tutorial. Please make sure to *save it* in that folder (it's fine that you're saving an empty script for now, you'll fill it in as we go).
:::

::: callout-note
This is a fairly hefty tutorial! If you find that it takes you longer to get through it, that's totally normal and no cause for concern.
:::

# Prerequisite knowledge

## Functions and variables

We learned a little about functions in the last tutorial. Pause for a moment and see whether you can come up with a good working definition of what a function is.

Here's mine: Functions take some inputs (known as arguments), apply some transformations, and then provide you with the output of those transformations. When we're working inside of a programming environment, nearly *everything* that you ask a computer to do is a function. For example, we're used to thinking about mathematical operations like `2+6` as "arithmetic". But under the hood, what is the computer actually doing?

```{r sum-operator-vs-func}
2+6
sum(2, 6)
```

As this example illustrates, the `+` operator acts as a shorthand for the function `sum()`. In this example, all of the numbers that we want to add are arguments to that function. This might seem like odd notation at first glance, but recall from your high school math class that these kinds of functions are actually pretty common in math. For example, you might've seen functions written out like this:\
$f(x) = x \times 3$, such that $f(5) = 15$)\
$f(x, y) = x^{y}$, such that $f(3, 2) = 9$

But beware! Functions don't always do what you think they'll do. For example, we'd naively expect that `mean(2, 6)` would result in the output `4`.

```{r mean-unexpected-output}
mean(2, 6)
```

Wait, what happened? To find out, we should consult the function documentation. To do this, type `?mean` into the R console (not your script) and see what you can figure out. Sometimes, the documentation is really helpful and you can figure out exactly how to use a function from reading it. Other times, you might need to look at the **Examples** at the bottom of the documentation to see sample code. And of course, there's always Google (though you should give [DuckDuckGo](https://duckduckgo.com/) a try if you care about personal privacy).

Here are some hints:

1.  You'll note that the argument `x` is supposed to be a **vector**.
    -   A vector is nothing more than a (one-dimensional) container for data.

    -   For example, let's say that I wanted to record the temperature inside my apartment every hour (an increasingly pressing concern now that we are headed into New England winter).

    -   I would have, in colloquial terms, a list of temperatures (list is actually a technical term in computer science, but let's not worry about that for now). If I wanted to represent that list of temperatures with my computer, I could do that using a single vector, which *contains* multiple numbers.
2.  In the examples, you'll note that there's another function used: `c()`. What does that function do?
3.  Still struggling? Try running `mean(c(2, 6))`. Now, explain why that works as expected.

But what's actually going on inside of a function? The best way to understand this is by creating a custom function.

```{r custom-mean}
custom_mean <- function(this_vector) {
  sum(this_vector) / length(this_vector)
}
```

You can see that this custom function takes one argument, `this_vector`. It then takes the sum of that vector, then divides by the vector length (i.e., the number of **elements** inside the vector; if the vector contains five numbers, it has a length of five).

Now let's **call** our custom function.

```{r call-custom-mean}
custom_mean(c(2, 6))

my_vector <- c(2, 6)
custom_mean(my_vector)
```

Ta-dah! We'll touch on one last thing before moving on. You'll note that our custom function took `this_vector` as an argument. But when we called it, we either fed it an unnamed vector `c(2, 6)`, or a vector that we'd assigned to the variable `my_vector`. What gives?

Remember that the point of a variable is that it can stand in for anything. In algebra, $x$ is commonly a variable that can take on the value of numbers ($x=9$), or even other variables ($x=3+9y+\frac{z}{2}$). So when we provide an argument to `custom_mean`, the value of that argument gets assigned to `this_vector`.

We can make this idea really explicit by using **named arguments**:

```{r using-named-args}
custom_mean(this_vector = c(2, 6))
custom_mean(this_vector = my_vector)
```

So you can see that the function is *creating* a new variable `this_vector`, which is assigned the value of the argument. When we call the function with a different argument, the value of `this_vector` changes to reflect whatever that argument is.

## Libraries

We covered libraries last time as well. What is a library? Try defining it in your mind. Now say that definition out loud and see if you like it.

If you feel uncertain, go back to the last tutorial and refresh your memory on what a library is, and what it's for.

Recall that you can always call a function from a library without loading the entire library. For example:

```{r namespace-example}
readr::read_csv(
  here::here("data", "countypres_2000-2016.csv"),
  n_max = 10
) %>%
  knitr::kable()
```

In our last tutorial, recall that we used that `library_name::function_name` syntax to visually differentiate between the functions `utils::read.csv` and `readr::read_csv`, which otherwise look very similar.

Of course, if you plan to use many functions from a library, or to repeatedly use the same (few) functions, it's oftentimes worth it to simply call the entire library. Now that you have a good working idea of what libraries are, load the following libraries: `here`, `readr`, and `dplyr`. If you don't remember how to do this, refer back to the last tutorial.

```{r load-libraries}
#| code-fold: true

library(here)
library(readr)
library(dplyr)
```

# dplyr

Today, we're going to focus all of our attention on **`dplyr`** (pronounced dee-plier, which stands for data-pliers, so named because this library allows you to [ply your data](https://duckduckgo.com/?q=pliers)). There are 7 key verbs you need to know, and the vast majority of your data wrangling will revolve around applying these 7 verbs creatively to get the results you want.

1.  `filter`: keep only some rows/observations
2.  `select`: keep only some columns/variables
3.  `mutate`: add a new column/variable
4.  `summarise`: reduce your dataset to a summarized version of a particular column/variable
5.  `arrange`: reorder your rows/observations
6.  `group_by`: perform operations based on some grouping of your data
7.  `join`: "merge" multiple dataframes together

## Read in the data

In the `data` folder, there is a datafile called `time_series_covid19_confirmed_US.csv`. Using `here` and `readr`, load that data and assign it to a variable named `covid`. There are a few ways you could do this, but you get bonus points if your solution uses the pipe operator `%>%`.

```{r read-covid-data}
#| code-fold: true
#| message: false

covid <- here(
  "data",
  "time_series_covid19_confirmed_US.csv"
) %>%
  read_csv()
```

This datafile contains confirmed US covid19 cases, and was downloaded from the [Johns Hopkins coronavirus dashboard](https://github.com/CSSEGISandData/COVID-19).

Let's take a quick look at this dataset. The `dplyr::slice` function returns only the first few rows so that you don't have to scroll through thousands of observations.

```{r peek-covid-data}
covid %>%
  slice(1:10)
```

## filter

Every row in this dataset represents one county (or equivalent) inside the United States. Depending on our interests, we might not want the full dataset. For example, if I'm an epidemiologist in California, I might want to **filter** the dataset so that I'm only looking at counties inside California.

That is exactly what `dplyr::filter` allows you to do. (For the time being, ignore `dplyr::select`. I'm only using it here so that I don't print off a billion columns again.)

```{r filter-example}
covid %>%
  filter(Province_State == "California") %>%
  select(
    fips = FIPS,
    county = Admin2,
    `9/18/20`:`9/24/20`
  ) %>%
  slice(1:10) %>%
  knitr::kable()
```

You'll note that there's some funky syntax going on here. Let's break that down.

`Province_State` is a variable inside the dataframe `covid`, which indexes what state the data come from. So we're asking to filter only the observations inside the variable `Province_State` that match `California`. The syntax `==` is two equal signs, and means *is exactly equal to*. To see how this works, try playing around with some of these examples:

```{r equality-examples}
"California" == "California"
"California" == "Texas"
"California" == 319
"California" == FALSE
210 == 210
210*2 == 420
TRUE == FALSE
(TRUE == FALSE) == FALSE
```

If that last one gives you a headache, try this instead: `("California" == "Texas) == FALSE`. And if that still makes you want to throw your computer out the window, ask yourself this: is it true that California is not equal to Texas?

`dplyr:filter` keeps only the observations that return `TRUE` when you test a particular **logical condition**. So let's say that we're only interested in counties where there were at least 1,000 cases on 9/14/20. How would we implement this? Try this on your own before seeing how I did it.

```{r filter-example-2}
#| code-fold: true

covid %>%
  filter(Province_State == "California") %>%
  select(
    fips = FIPS,
    county = Admin2,
    `9/18/20`:`9/24/20`
  ) %>%
  filter(`9/24/20` >= 1000) %>%
  slice(1:10) %>%
  knitr::kable()
```

You may recall learning about equalities and inequalities in your grade-school math classes. As a refresher, here are the basic logical operators you need to know:

1.  `==`: exactly equal to
2.  `!=`: not equal to
3.  `>`: greater than
4.  `>=`: at least (greater than or equal to)
5.  `<`: less than
6.  `<=`: at most (less than or equal to)

## select

Using `dplyr::filter`, we were able to selectively keep certain rows/observations. What if we want to selectively keep certain columns/variables? For example, I might not be interested in knowing the full history of confirmed covid cases. I might only be interested in knowing what happened in the span of a select few days. This is where `dplyr::select` comes in. We've already seen an example of this in action, but let's now dissect how it works.

For the sake of keeping this explanation simple, we'll focus our attention on Yolo County in California using `dplyr::filter`. Then we'll pull out only the FIPS code (a unique numerical identifier for each county), the county name, and records of cases between September 18-24. In this particular dataset, we can see that the FIPS code is stored in a column called `FIPS` and that the county name is in `Admin2`.

```{r select-example}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(FIPS, Admin2, `9/18/20`:`9/24/20`) %>%
  knitr::kable()
```

You might be wondering what's going on with the syntax `` `9/18/20`:`9/24/20` ``. That weird symbol is **not** an apostrophe, but rather a **backtick** (truly, a horrible name).

On a standard US keyboard, you can find this on the same key as the **tilde** (i.e., the squiggly `~`). In R, they are interpreted as a **literal** when evaluating nonstandard variable names. Typically, you're not allowed to use variable names that start with a number, and dataframe columns are no exception. However, in this case, the data in its original format uses dates as column names, so we need to tell R to interpret `9/18/20` as a literal. Otherwise, R would interpret it as "9 divided by 18 divided by 20". Below, you can see for yourself that this didn't work as expected.

```{r no-backtick-example}
#| error = TRUE

covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(FIPS, Admin2, 9/18/20:9/24/20) %>%
  knitr::kable()
```

The second thing you might be curious about is what `:` is doing. The syntax `select(from_here:to_there)` tells R to select all of the columns between `from_here` and `to_there`.

You can also use `dplyr::select` to rename your variables on-the-fly! For example, we might be interested in renaming `FIPS` to `fips` so that we can be lazy about typing the variable name. We might also want to rename `Admin2` to `county` so that our variable name better reflects what data is encoded in that column. Below, you can see an example of how to do this.

```{r select-rename}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(
    fips = FIPS,
    county = Admin2,
    `9/18/20`:`9/24/20`
  ) %>%
  knitr::kable()
```

Doing that can be a little more efficient than first selecting columns, then renaming them, as you can see below.

```{r rename-explicitly}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(FIPS, Admin2, `9/18/20`:`9/24/20`) %>%
  rename(fips = FIPS, county = Admin2) %>%
  knitr::kable()
```

Finally, you can use `dplyr::select` to *get rid* of columns you don't want anymore.

```{r select-drop-cols}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(
    state = Province_State,
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  # Actually, we've changed our mind! We don't want the state variable anymore
  # Get rid of it using the minus sign
  select(-state) %>%
  knitr::kable()
```

## mutate

So far, we've wrangled data in the sense that we're only keeping certain parts of the overall dataframe that we want to retain. We can either filter certain observations/rows, or we can select certain variables/columns.

Now, let's turn our attention to creating new variables. One of the standard assumptions of linear models (e.g., statistical tests like t-tests, correlations, and regressions) is that your data come from a normal distribution (sort of -- this isn't strictly true but we'll ignore this for now).

However, count data (e.g., the number of covid19 cases) are almost never normally distributed. A common solution to this is to apply some sort of transformation to the counts before analyzing them, like taking the logarithm or square root. (Whether this is an appropriate statistical procedure is beyond the scope of this tutorial; we're doing it just for practice.)

This is where `dplyr::mutate` comes in. The verb mutate is a bit quirky, but it communicates the point that you're transforming (mutating) your data in some way to make it different from when it started. Let's look at an annotated example to illustrate.

```{r mutate-example}
# Start off with the original dataframe
covid %>%
  # Keep only observations from California
  filter(Province_State == "California") %>%
  # Let's keep only the first 20 counties
  slice(1:20) %>%
  # Keep only a few columns of interest
  select(
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  # Now apply a logarithmic transformation
  mutate(latest_cases_log = log(latest_cases)) %>%
  knitr::kable()
```

So you can see that this created a new column named `latest_cases_log`, which took all the values inside `latest_cases` and applied a function (in this case, `log()`) to all of those values. Note that you could also *overwrite* an existing column, depending on how you name it. For example, we can replace `latest_cases` with a version that contains log-transformed counts.

```{r mutate-overwrite}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  mutate(latest_cases = log(latest_cases)) %>%
  knitr::kable()
```

And, just to show that this is a general phenomenon, let's overwrite the FIPS column instead. So, be careful about using `dplyr::mutate`, as it can lead to overwriting data if you reuse an existing variable name!

```{r mutate-overwrite-2}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  mutate(fips = log(latest_cases)) %>%
  knitr::kable()
```

Note that `dplyr::mutate` creates a new variable containing as many observations as in the dataframe used to create that variable. You should also know that you can also create a column containing a single value for every single one of those observations.

For the sake of demonstration, let's pretend that the covid19 counts are stored in different CSV files according to what state they come from. After opening up the California counts, we would then want to create a column indicating that these particular data come from California. How would we do this?

```{r mutate-single-value-col}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  # Pretend we got this data from a CSV containing no state info
  select(
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  # All rows of this column now contain a singular value
  mutate(state = "California") %>%
  knitr::kable()
```

Finally, you might be interesting in applying the same function to multiple columns. For example, you might be interested in applying a log transformation not just to the covid19 counts on 9/24/20, but for that entire week. We could do that using a function called `dplyr::across`. This function takes two arguments: one specifying which columns you want to mutate, and another specifying what you want to do with those columns.

```{r mutate-across}
covid %>%
  select(
    state = Province_State,
    county = Admin2,
    `9/18/20`:`9/24/20`
  ) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  mutate(
    across(
      .cols = `9/18/20`:`9/24/20`,
      .fns = ~log(.x + 1)
    )
  ) %>%
  knitr::kable()
```

Let's take a second to talk about that second argument, `.fns`. The syntax we're using is kind of funky, and it's not immediately intuitive what we're doing here: `.fns = ~log(.x + 1)`.

This is known as a **lambda function**, which is actually pretty advanced and way outside the scope of this workshop. The need-to-know takeaway is that you can create a new function on-the-fly, which is not named (contrast this against the `my_mean` function we built earlier), and only used once.

The squiggly `~` (called a tilde) signals that you're creating a lambda function, and the phrase `.x` refers to the data you're passing to the lambda function. In this case, `.x` refers to each of the columns that you want to mutate, and the syntax specifies that we want to take the logarithm of each column after adding 1 to its covid19 count. This is because $log(0) = -\infty$. To avoid getting this as a result, we add the constant 1 so that when there are zero cases, we get the easier-to-work-with result $log(1) = 0$.

Still feel like lambda functions are inscrutable? No worries, this is a level of programming abstraction that's really hard to wrap your mind around. I'll show you one final example of how we'd build a normal function that does the same thing, which might help you make the connection. And if you don't get it, it's not the end of the world.

```{r lambda-func-example}
# Custom function. Note similarity w/ the lambda function we used earlier!
avoid_log_trap <- function(x) {
  log(x + 1)
}

covid %>%
  select(
    state = Province_State,
    county = Admin2,
    `9/18/20`:`9/24/20`
  ) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  mutate(
    across(
      .cols = `9/18/20`:`9/24/20`,
      # Everything is the same up to this point
      # Now we replace the lambda function w/ our custom function
      .fns = avoid_log_trap
    )
  ) %>%
  knitr::kable()
```

## summarise

Sometimes, we don't want to compute a value for every observation, but rather a single value **summarizing** over observations. For example, given a vector of numbers, we might be interested in knowing a summary statistic about this vector: its mean, standard deviation, min/max, sum, and so on. This is what the function `dplyr::summarise` is for.

As a sidenote: If you're from the US, you're probably more used to the spelling *summarize*. There is a corresponding function called `dplyr::summarize` that literally just calls `dplyr::summarise` under the hood. Either is fine, but pick one and stick to it. The "British English" spelling is the default because one of the main developers behind the tidyverse is from New Zealand.

So for example, we might be interested in knowing the total number of confirmed covid19 cases in California on a particular day. How would we compute that?

```{r summarise-example}
covid %>%
  filter(Province_State == "California") %>%
  select(
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  summarise(total_cases = sum(latest_cases)) %>%
  knitr::kable()
```

And there we have it. You can see that it operates in a way that's very similar to `dplyr::mutate`.

That goes for the use of `dplyr::across` as well. If we're interested in getting the daily sum of all Californian covid19 cases, we can do this pretty easily.

```{r summarise-across}
covid %>%
  filter(Province_State == "California") %>%
  select(
    fips = FIPS,
    county = Admin2,
    `9/18/20`:`9/24/20`
  ) %>%
  summarise(
    across(
      .cols = `9/18/20`:`9/24/20`,
      .fns = sum
    )
  ) %>%
  knitr::kable()
```

## arrange

It is oftentimes useful to reorder your observations. You can do this easily using `dplyr::arrange`. For example, we might want to reorder our dataframe according to the number of covid19 cases.

```{r arrange-ascend}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  arrange(latest_cases) %>%
  knitr::kable()
```

Similarly, we could arrange the dataframe according to county names. Just for demonstration, we'll do it in reverse alphabetical order using the function `desc`.

```{r arrange-descend}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(
    fips = FIPS,
    county = Admin2,
    latest_cases = `9/24/20`
  ) %>%
  arrange(desc(county)) %>%
  knitr::kable()
```

## group_by

Oftentimes, we have dataframes where observations can be grouped together. The `dplyr::group_by` function doesn't do anything on its own, but it's extremely powerful when you use it in conjunction with other functions.

For example, in our original `covid` dataframe, we have data for every county in every state in the US. We could be interested in summarizing the total number of covid19 cases at the state level, which requires grouping together observations from counties that belong to the same state.

```{r group-by-summarise}
covid %>%
  rename(
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  group_by(state) %>%
  summarise(n_cases = sum(latest_cases)) %>%
  # When you're done manipulating grouped data, remember to ungroup!
  # Otherwise, you may get unexpected (and hard-to-diagnose) problems later on.
  ungroup() %>%
  arrange(desc(n_cases)) %>%
  slice(1:10) %>%
  knitr::kable()
```

::: callout-warning
Make it a habit to call `ungroup` as soon as you're done working with grouped data! Otherwise, you are likely to get unexpected (and sometimes hard-to-diagnose) problems later on.
:::

## join

As data scientists, we want to know not only how many cases there are, but what kinds of factors *predict* why some places have more cases than others. This requires us to incorporate more data into our analyses.

For example, it's not particularly surprising that California and Texas have the most number of covid19 cases. Why? Well, they just happen to have the biggest populations in the entire US. We might instead want to know whether, *proportional to their population size*, these states are doing worse than, say, North Dakota. We can't do that without having additional data about each county's population.

We might also be interested in understanding whether counties are doing systematically better/worse depending on how urban they are.

And finally, we know that Republican-leaning and Democrat-leaning areas have taken different public health approaches to combating the virus. A natural question we might have is whether certain kinds of public health policies/philosophies are associated with covid19 prevalence. So, if we had an index of a county's political leanings, we could see whether that has a relationship with how well/poorly a given county is managing its covid19 cases.

To answer these questions, we'll need to take various pieces of information stored in different dataframes, and find a way to **join** them all together. So, the final wrangling functions we'll discuss today are all the variants of mutating joins:

1.  `dplyr::inner_join`
2.  `dplyr::left_join`
3.  `dplyr::right_join`
4.  `dplyr::full_join`

In order to demonstrate the power of joining, we'll need to pull in a few other datasets. First, let's read in the [CDC NCHS' urban-rural classification scheme](https://www.cdc.gov/nchs/data_access/urban_rural.htm), which classifies every county along a spectrum of urbanicity (from rural to metro). When we take a look at these data, we notice that the `urbanicity` dataset contains a FIPS identifier, just like the `covid` dataset.

```{r load-urbanicity}
#| message=FALSE

urbanicity <- here("data", "NCHSURCodes2013.xlsx") %>%
  readxl::read_excel(na = c(".")) %>%
  janitor::clean_names() %>%
  select(
    fips_code,
    urbanicity = x2013_code,
    population = county_2012_pop
  )

urbanicity %>%
  slice(1:5) %>%
  knitr::kable()
```

Now, let's pull in county-level voting data from the 2016 election, downloaded from [MIT's Election Lab](https://electionlab.mit.edu/data). In their original form, these data simply give you the total number of votes for Trump, and the total number of votes for Clinton. Our goal is to create a single-number index of how much a particular county voted for Trump over Clinton.

One method for doing this is to find the ratio $\text{Trump votes} \over \text{Clinton votes}$. This ratio can be interpreted such that 1 = same number of people voted for Trump and Clinton, and numbers greater than 1 indicate more preference for Trump (e.g., a ratio of 5 would mean five times as many people voted for Trump than Clinton).

Take a few minutes to look through the code below, and try to verbalize what each line of code is doing. Then, run each line one-by-one so that you can see what's happening at each processing step. In particular, see if you understand what's happening with the combination of `group_by`, `mutate`, and `first`.

```{r load-elections}
#| message=FALSE

elections <- here("Data", "countypres_2000-2016.csv") %>%
  read_csv() %>%
  filter(year == 2016) %>%
  filter(party %in% c("democrat", "republican")) %>%
  group_by(state, county, FIPS) %>%
  mutate(lean_republican = candidatevotes / first(candidatevotes)) %>%
  ungroup() %>%
  filter(party == "republican") %>%
  select(state, county, FIPS, lean_republican)
```

Let's take a look at this wrangled dataset. Again, there's that old friend of ours, the FIPS code.

```{r peek-elections}
elections %>%
  slice(1:5) %>%
  knitr::kable()
```

### left join

The idea behind all join operations is that your datasets contain **key-value** pairs. If two datasets share the same key, then you can use that common key to merge/join the values inside both datasets.

Let's make this concrete using an example. Run the following, and take note of the message that's printed to the console.

```{r left-join-example}

# Start with the covid count data
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  # Join with the election data
  left_join(elections) %>%
  knitr::kable()
```

The `covid` and `elections` datasets had a few key variables in common. This is reflected in the message saying `Joining, by = c("FIPS", "county", "state")`, which indicates that these three columns were present in both `covid` and `elections`.

So how does joining work, exactly? Let's try thinking about how we would do this manually.

First, we would look to see whether the two datasets have any columns with the same names. In this case, we find that both datasets have columns named `FIPS`, `county`, and `state`. These become our key columns.

We're doing a **left join** here, so we start with our "left-hand-side" (LHS) dataset. In our, that's `covid`. (If it helps, you can imagine that we wrote the last line as something like `left_join(covid, elections`, which makes it clearer why the `covid` dataset is the "left" dataset.)

We look through the LHS dataset to find unique keys. Here, unique keys of `FIPS` include `6001`, `6003`, `6005`, and so on. We then check the "right-hand-side" (RHS) dataset (`elections` here) to see if there's a corresponding `FIPS==6001`, `FIPS==6003`, `FIPS==6005`, and so on. If there is, then join/merge all other columns from RHS to LHS. This explanation glosses over the fact that there are multiple key columns, but you get the general idea for how this sort of algorithm works.

Therefore, when the keys match between LHS and RHS, all unique values in RHS get merged to the corresponding entities in LHS. That's how we ended up with values for *both* `latest_cases` (unique to `covid`) and `lean_republican` (unique to `elections`) in a new dataframe.

What happens when a particular key exists in RHS, but not in LHS? To find out, let's create a scenario where this is true.

```{r left-join-no-lhs}
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  # This is new!
  filter(FIPS != 6001) %>%
  left_join(elections) %>%
  knitr::kable()
```

We can see that since `FIPS==6001` no longer exists in LHS, the values associated with `FIPS==6001` in RHS get dropped. In fact, we've already seen this in action. The `elections` dataset contains observations from thousands of counties, yet when we previously performed our left join, we got back a dataframe containing only 10 observations. We didn't think anything of it at the time, but this latest example demonstrates that this is a specific behavior associated with left joins. We'll later see that other kinds of joins work very differently.

Okay, so what about the opposite scenario: when a particular key exists in LHS, but not in RHS?

```{r left-join-no-rhs}
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  # This is new!
  left_join(elections %>% filter(FIPS != 6001)) %>%
  knitr::kable()
```

Because we start with LHS, we retain all observations from LHS, even if there's no corresponding data in RHS.

One last exercise for left joins: if you look at the columns in `urbanicity`, you'll note that we have FIPS codes available, but in a column named `fips_code` instead of `FIPS`. How can we get these dataframes to join properly?

```{r left-join-named-vector}

# Start with the covid count data
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  # Join with the election data
  left_join(elections) %>%
  # Join with the population data
  left_join(urbanicity, by = c("FIPS" = "fips_code")) %>%
  knitr::kable()
```

Let's break this down. Even though we had FIPS codes in all three datasets, the column name was different in the `urbanicity` dataset. So when we joined this dataset with the others, we had to manually specify that the column `FIPS` in `covid` and `elections` contained the same data as the column `fips_code` in `elections`.

We did this using the argument `by = ...`, which took a vector (created by `c()`). This vector is a little bit different from the vectors we've seen before, where we stored a simple "list" of elements (e.g., `c(1, 2, 3, 4)`). This is known as a **named vector**, because every element in the vector has a corresponding "name". It might help to think of a named vector as a *dictionary* (e.g., try typing `` c(`1`="one fish", `2`="two fish", `3`="red fish", `4`="blue fish") `` into the console).

### right join

Now, let's do the same thing, but instead of using `dplyr::left_join`, we'll use `dplyr::right_join`. Note this important feature of the data: we're only going to be keeping the first 5 rows of `covid`, but then we'll try joining that 5-row dataset with the full `elections` dataset.

```{r right-join-example}
# Start with the covid count data
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state == "California") %>%
  slice(1:5) %>%
  # Join with the election data
  right_join(elections) %>%
  # Restrict to first few observations so we don't print 1000s of rows
  slice(1:10) %>%
  knitr::kable()
```

You can see that for the first five rows, the join worked as expected. However, when we look at the next five rows, there's no data in the `latest_cases` column! This is because `dplyr::left_join` keeps all rows from LHS, which was 5 rows from `covid`. On the other hand, `dplyr::right_join` keeps all rows from RHS, which is all 3158 rows of `elections` (we restricted the output to 10 observations at the very end so that you don't have to scroll through 1000s of observations; try running this without the final line to verify that all rows of `elections` ended up in the output dataframe).

::: callout-tip
In practice, I almost always use left joins instead of right joins. This is because I'm starting with a given dataframe, then iteratively using pipes to manipulate it. So when I'm joining a RHS dataframe to a LHS dataframe that I've already been working with, I typically want to keep all of the data from the LHS dataframe.
:::

### inner join

What if you only wanted to keep rows where there was a match in the LHS **and** RHS dataframes? To illustrate, let's take a look at a geographical location that has entries in `covid`, but not in `elections`. Since Puerto Ricans are not eligible to vote in US elections, there are no rows in `elections` corresponding to Puerto Rico, as we can see below.

```{r verify-no-pr-votes}
elections %>%
  filter(state == "Puerto Rico") %>%
  knitr::kable()
```

What does the left join do when there are rows in LHS (`covid`), but not RHS (`elections`)? We've seen an example like this before, so take a second to predict what output you'd expect when joining those dataframes. Now that you've made a prediction, let's see if you're right. For illustration, we'll compare the output for Puerto Rico vs California. Because left joins keep all rows from LHS, the output dataframe keeps those rows and produces `NA`s when there is no corresponding data in RHS.

```{r pr-nan-join}
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state %in% c("Puerto Rico", "California")) %>%
  # Note how the combination of group_by and slice results in an output where
  # we keep the first five rows from *both* Puerto Rico and California!
  group_by(state) %>%
  slice(1:5) %>%
  ungroup() %>%
  left_join(elections) %>%
  knitr::kable()
```

Sometimes, you might only want to keep data when there are matching rows in LHS and RHS. In that case, you'd want to use `dplyr::inner_join`. Let's run the same code as before, but replace `dplyr::left_join` with `dplyr::inner_join`.

```{r inner-join-example}
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state %in% c("Puerto Rico", "California")) %>%
  group_by(state) %>%
  slice(1:5) %>%
  ungroup() %>%
  inner_join(elections) %>%
  knitr::kable()
```

We can see from this output that only the rows from California were kept.

::: callout-warning
In the past, I've gotten myself in trouble using inner joins. This is easy to do when you assume there are matching rows between the LHS and RHS dataframes, when in fact this is not true. Since this kind of join is the most likely to (silently!) drop data, make sure you build in checks to make sure the join happens as expected.
:::

### full join

Finally, there are times when you might want to keep **all** rows from LHS and RHS. To avoid printing out the full output, I'll restrict `covid` so that it only contains 5 counties from Puerto Rico and California, and `elections` so that it only contains 5 counties from Alabama. In doing so, I've made it so that there are **no** matching rows between LHS and RHS. We can still join these together and keep all possible rows by using `dplyr::full_join`.

```{r full-join-example}
covid %>%
  select(
    FIPS,
    county = Admin2,
    state = Province_State,
    latest_cases = `9/24/20`
  ) %>%
  filter(state %in% c("Puerto Rico", "California")) %>%
  group_by(state) %>%
  slice(1:5) %>%
  ungroup() %>%
  # For illustration, restricting elections to 5 Alabama counties
  full_join(
    elections %>%
      filter(state == "Alabama") %>%
      slice(1:5)
  ) %>%
  knitr::kable()
```

# Exercises

A general note: these are hard exercises, and you won't necessarily find the answers in this tutorial. Learning to Google (or, if you really care about privacy, [DuckDuckGo](https://duckduckgo.com/)) answers to your coding questions is itself an essential skill. So, be patient with yourself. Struggling through these exercises will help you understand this material at a deeper level.

## Exercise 1

The Vera Institute of Justice has a publicly-available dataset on [county-level incarceration trends over time](https://github.com/vera-institute/incarceration-trends). You can find it in `data/incarceration_trends.csv`. To give you a flavor for what real-world data wrangling is like, you're going to perform lots of different operations with this dataset.

Read in the data (using `here` and `readr`), and assign it to a variable named `incarceration`.

Create a new dataframe called `ca_jail`, which keeps only the **2018** records from **California**. You can do this in two lines of code, but as a challenge, see if you can do it in one (hint: look up the documentation for `base::Logic`).

For our purposes, there are only a handful of variables we want to keep: the FIPS identifier, each county's total population (`total_pop`), and the total jail population (for this exercise, we won't be looking at prison populations).

It's hard to compare counties' jail populations because every county has a different population size. Find some way to address this problem. A sensible name for the column you create might be `prop_jail` (prop = proportion).

## Exercise 2

Let's say a colleague of yours is interested in how a county's political leanings affect its tendency to jail people. Luckily, we've just been working with a dataset that can address this. Create a version of `ca_jail` that incorporates relevant data from `elections`. Rearrange the rows of this dataframe (using code!) according to the `lean_republican` ratio. By looking at the first 10 rows and last 10 rows, provide a qualitative answer for your colleague. Statistical analyses will have to wait until later, when we learn how to analyze quantitative data.

## Exercise 3

We might be able to do better. Create a variable indicating whether a county's `lean_republican` ratio is greater than 1 (meaning that more people voted for Trump than for Clinton). A sensible name for this column might be something like `more_trump`.

Based on whether counties had more Trump or Clinton voters in 2016, summarize **both** the mean and standard deviation of `prop_jail`.

Two hints may help you accomplish this.

1.  Your output table should have two rows.
2.  The function `mean` doesn't like it when there are `NA`s in the data, and you'll note that FIPS area 6003 has no data about jail population. Take a look at the documentation for this function to see what can be done about this.

## Exercise 4

Now that you've done this for California, do it for the entire USA (i.e., not state-by-state, but literally summarizing for the entire country). Are the results what you expected? How do you interpret these results?

## Exercise 5

Following up on that last prompt, it may be instructive to aggregate not at the level of the entire USA, but at the state level. You might find it instructive to read this NPR article entitled ["How Louisiana Became The World's Prison Capital"](https://www.npr.org/2012/06/05/154352977/how-louisiana-became-the-worlds-prison-capital). Does this change your interpretation? Why?

## Reminder

Practice using version control! Once you're at a good stopping point, **commit** your changes and then **push** those commits to GitHub.
